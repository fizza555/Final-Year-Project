import cv2
from mtcnn import MTCNN
from flask import Flask, request, jsonify
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.preprocessing import LabelEncoder
import librosa
import warnings
from tensorflow.keras.applications import VGG16
from moviepy.editor import VideoFileClip
from flask_cors import CORS
import cv2
from mtcnn import MTCNN
from flask import Flask, request, jsonify
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.preprocessing import LabelEncoder
import librosa
import warnings
from tensorflow.keras.applications import VGG16
from moviepy.editor import VideoFileClip
from flask_cors import CORS

# Initialize Flask app
app = Flask(__name__)
CORS(app)

# To ignore only FutureWarnings
warnings.filterwarnings("ignore")


# Initialize MTCNN face detector
face_detector = MTCNN()


def extract_audio(video_path, audio_path):
    video = VideoFileClip(video_path)
    try:
        audio = video.audio
        audio.write_audiofile(audio_path, verbose=False, logger=None)
    finally:
        video.close()  # Ensure the video file is closed properly

def extract_audio1(video_path, audio_path):
    video = VideoFileClip(video_path)
    try:
        audio = video.audio
        audio.write_audiofile(audio_path, verbose=False, logger=None)
    finally:
        video.close()  # Ensure the video file is closed properly

# Function to check if audio is present in the video
def check_audio_in_video(video_path):
    video = VideoFileClip(video_path)
    try:
        return video.audio is not None
    finally:
        video.close()  # Ensure the video file is closed properly

def check_audio_in_video_processed(video_path):
    video = VideoFileClip(video_path)
    try:
        return video.audio is not None
    finally:
        video.close()  # Ensure the video file is closed properly




def generate_spectrogram(audio_path):
    y, sr = librosa.load(audio_path, sr=None)  # Load audio with original sample rate
    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
    return D, sr


def create_video_model(input_shape):
    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    vgg16.trainable = False  # Freeze the base VGG16 layers

    # Sequential model for processing temporal frame data
    model = models.Sequential([
        layers.TimeDistributed(vgg16, input_shape=input_shape),
        layers.TimeDistributed(layers.Flatten()),
        layers.LSTM(64, return_sequences=False),  # LSTM for temporal features
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.4),  # Dropout for regularization
        layers.BatchNormalization(),
        layers.Dense(1, activation='sigmoid')  # Classification layer
    ])
    return model

def create_audio_model(input_shape):
    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)
    vgg16.trainable = False  # Freeze base layers

    model = models.Sequential([
        vgg16,
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.4),  # Dropout for regularization
        layers.BatchNormalization(),
        layers.Dense(1, activation='sigmoid')  # Final classification layer
    ])
    return model
# Function to extract the spectrogram from the audio and resize it to match VGG16 input
def extract_audio_features_from_video(video_path):
    audio_path = "temp_audio.wav"  # Temporary file path for audio
    extract_audio(video_path, audio_path)
    D, sr = generate_spectrogram(audio_path)
    resized_spectrogram = cv2.resize(D, (224, 224))
    resized_spectrogram = np.stack([resized_spectrogram] * 3, axis=-1)  # Convert to 3 channels (for VGG16 input)

    # Clean up temp audio file
    if os.path.exists(audio_path):
        os.remove(audio_path)

    return resized_spectrogram


# Function to extract video frames
def extract_video_frames(video_path, max_frames=200, frame_shape=(224, 224)):
    frames = []
    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Resize and normalize the frame
        frame = cv2.resize(frame, frame_shape)
        frame = frame / 255.0  # Normalize pixel values
        frames.append(frame)

        frame_count += 1
        if frame_count >= max_frames:
            break

    cap.release()

    # Padding if there are not enough frames
    if len(frames) < max_frames:
        padding = [np.zeros((224, 224, 3)) for _ in range(max_frames - len(frames))]
        frames.extend(padding)

    return np.array(frames)


# Function to create the model with VGG16 for both video and audio streams
def create_model_with_vgg16(input_video_shape, input_audio_shape, num_classes):
    video_input = layers.Input(shape=input_video_shape)
    audio_input = layers.Input(shape=input_audio_shape)

    # Load VGG16 for video input (feature extraction)
    vgg16_video = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    vgg16_video.trainable = False  # Freeze base layers

    # Video stream
    video_features = layers.TimeDistributed(vgg16_video)(video_input)
    video_features = layers.TimeDistributed(layers.Flatten())(video_features)
    video_features = layers.LSTM(64, return_sequences=False)(video_features)
    video_features = layers.Dense(128, activation='relu')(video_features)
    video_features = layers.Dropout(0.4)(video_features)  # Increased dropout to reduce overfitting
    video_features = layers.BatchNormalization()(video_features)

    # Load VGG16 for audio input (feature extraction)
    vgg16_audio = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    vgg16_audio.trainable = False  # Freeze base layers

    # Audio stream
    audio_features = vgg16_audio(audio_input)
    audio_features = layers.Flatten()(audio_features)
    audio_features = layers.Dense(128, activation='relu')(audio_features)
    audio_features = layers.Dropout(0.4)(audio_features)  # Increased dropout to reduce overfitting
    audio_features = layers.BatchNormalization()(audio_features)

    # Concatenating both audio and video features
    concatenated = layers.Concatenate()([video_features, audio_features])
    output = layers.Dense(num_classes, activation='softmax')(concatenated)

    model = models.Model(inputs=[video_input, audio_input], outputs=output)
    return model


# Function to load the model with pre-trained weights
def load_pretrained_model(model_path, input_video_shape, input_audio_shape, num_classes):
    model = create_model_with_vgg16(input_video_shape, input_audio_shape, num_classes)
    model.load_weights(model_path)
    return model


# Function to classify a single video
def classify_video(video_path, model, label_encoder, max_frames=80):
    # Extract video frames and audio features
    video_frames = extract_video_frames(video_path, max_frames=max_frames)
    audio_features = extract_audio_features_from_video(video_path)

    # Ensure correct input shapes
    video_frames = np.expand_dims(video_frames, axis=0)  # Shape: (1, max_frames, 224, 224, 3)
    audio_features = np.expand_dims(audio_features, axis=0)  # Shape as needed

    # Predict the class
    predictions = model.predict([video_frames, audio_features])
    predicted_class_avg = np.argmax(predictions, axis=1)
    # Decode the predicted class index back to the original label
    predicted_label = label_encoder.inverse_transform(prediction_labels)
    return predicted_label[0], predictions


# Load the label encoder
def load_label_encoder(csv_path):
    train_df = pd.read_csv(csv_path)
    train_labels = train_df['label'].tolist()

    # Encode class labels
    label_encoder = LabelEncoder()
    label_encoder.fit(train_labels)
    return label_encoder

def load_label_encoder_test(csv_path):
    train_df = pd.read_csv(csv_path)
    train_labels = train_df['label'].tolist()

    # Encode class labels
    label_encoder = LabelEncoder()
    label_encoder.fit(train_labels)
    return label_encoder


# Function to check if the video contains a face
def check_face_in_video(video_path, max_frames=20):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return False  # Could not open video

    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Convert frame to RGB for face detection
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Detect faces in the frame
        faces = face_detector.detect_faces(rgb_frame)
        if len(faces) > 0:
            cap.release()
            return True  # Face detected

        frame_count += 1
        if frame_count >= max_frames:
            break

    cap.release()
    return False  # No face detected


# Define input shapes and parameters
input_video_shape = (80, 224, 224, 3)  # Assuming max_frames=200
input_audio_shape = (224, 224, 3)


# API route for video upload and classification
@app.route('/api/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400

    continent = request.form.get('continent')  # Get continent from the frontend
    if continent is None:
        return jsonify({'error': 'Continent not specified'}), 400

    # Set model and CSV paths based on the continent
    model_path, csv_path = None, None
    if continent == 'Africa':
        model_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\final_modelAfrican.keras'
        csv_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\train_videos_labelsAfrican.csv'
    elif continent == 'East Asia':
        model_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\final_modelAsian_East.keras'
        csv_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\train_videos_labelsAsian_East.csv'
    elif continent == 'South Asia':
        model_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\final_modelAsian_South.keras'
        csv_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\train_videos_labelsAsian_South.csv'
    elif continent == 'Caucasian American':
        model_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\final_modelCaucasian_American.keras'
        csv_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\train_videos_labelsCaucasian_American.csv'
    elif continent == 'Caucasian European':
        model_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\final_modelCaucasian_European.keras'
        csv_path = 'C:\\Users\\FIZZA\\PycharmProjects\\FYP\\train_videos_labelsCaucasian_European.csv'
    else:
        return jsonify({'error': 'Invalid continent specified'}), 400

    # Load the pre-trained model and label encoder
    label_encoder = load_label_encoder(csv_path)
    model = load_pretrained_model(model_path, input_video_shape, input_audio_shape, len(label_encoder.classes_))

    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400

    video_path = 'temp_video.mp4'
    file.save(video_path)

    # Check if the video has audio
    if not check_audio_in_video(video_path):
        # Remove the temporary video file
        os.remove(video_path)
        return jsonify({
                           'error': 'No audio found in the video, please attach a video file that has audio'}), 400  # Error status 400 for no audio

    # Check if the video contains a face
    if not check_face_in_video(video_path):
        # Remove the temporary video file
        os.remove(video_path)
        return jsonify({
                           'error': 'No face found in the video, please attach a video with a face visible'}), 400  # Error status 400 for no face

    # Only classify if both checks are passed
    predicted_label, predictions = classify_video(video_path, model, label_encoder)

    # Remove the temporary video file
    if os.path.exists(video_path):
        os.remove(video_path)

    # Send response back to frontend
    return jsonify({'predicted_label': predicted_label, 'confidence': float(np.max(predictions))})


# Run the Flask app
if __name__ == '__main__':
    app.run(debug=True)
